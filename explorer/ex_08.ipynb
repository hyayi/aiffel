{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.3\n",
      "0.5.2\n",
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import konlpy\n",
    "import gensim\n",
    "\n",
    "print(pandas.__version__)\n",
    "print(konlpy.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=15000):\n",
    "  # 데이터 전처리(가공)후에 다시 train, test 데이터로 나눠주기 위해 원래 train_data의 크기를 기억\n",
    "  len_train = len(train_data)\n",
    "  \n",
    "  all_data = pd.concat([train_data, test_data], axis=0, ignore_index = True)\n",
    "  # 중복치와 결측치 제거\n",
    "  all_data = all_data.drop_duplicates('document', keep='first')\n",
    "  all_data = all_data.dropna(axis=0)\n",
    "  \n",
    "  # 후기 문장들을 sentence_list에 list로 저장\n",
    "  sentence_list = list(all_data['document'])\n",
    "  # 형태소로 나눈 문장들을 리스트에 따로 저장\n",
    "  token_list = []\n",
    "  for sentence in sentence_list:\n",
    "    tokenize_sentence = tokenizer.morphs(sentence)\n",
    "    tokenize_sentence = [word for word in tokenize_sentence if word not in stopwords] # 불용어 제거\n",
    "    token_list.append(tokenize_sentence)\n",
    "  # 토큰화한 문장들을 train, test로 나눠줌\n",
    "  x_train = token_list[:len_train]\n",
    "  x_test = token_list[len_train:]\n",
    "  \n",
    "  words = np.concatenate(x_train).tolist()\n",
    "  counter = Counter(words)\n",
    "  counter = counter.most_common(15000 - 4) # <pad>, <bos>, <unk>, <unused> 토큰을 추가해야하기 때문에 4개를 빼줌\n",
    "  vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "  \n",
    "  word2index = {word : index for index, word in enumerate(vocab)} # 단어를 정수 인덱스로 바꿔줌\n",
    "  \n",
    "  def wordlist_to_indexlist(wordlist):\n",
    "    return [word2index[word] if word in word2index else word2index['<UNK>'] for word in wordlist]\n",
    "  \n",
    "  x_train_index = list(map(wordlist_to_indexlist, x_train))\n",
    "  x_test_index = list(map(wordlist_to_indexlist, x_test))\n",
    "  \n",
    "  return x_train_index, np.array(list(all_data['label'][:len_train])), x_test_index, np.array(list(all_data['label'][len_train:])), word2index, x_train, x_test\n",
    "  \n",
    "x_train, y_train, x_test, y_test, word2index, x_train_word, x_test_word = load_data(train_data, test_data)\n",
    "index2word = {index : word for word, index in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  16.0182376132783\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.842655296434431\n",
      "pad_sequences maxlen :  41\n",
      "전체 문장의 0.9340557100486782%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "\n",
    "# 텍스트데이터 문장길이의 리스트를 생성\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산\n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 최대 길이를 (평균 + 2 * 표준편차) 로 한다면 남는 문장의 비율은?\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 41)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 문장의 마지막 입력이 최종 state에 영향을 가장 크게 미치기 때문에 패딩 적용은 pre(앞쪽)으로 해줍니다.\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word2index['<PAD>'],\n",
    "                                                        padding = 'pre',\n",
    "                                                        maxlen = maxlen)\n",
    " \n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word2index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135000, 41) (135000,)\n"
     ]
    }
   ],
   "source": [
    "val_x = x_train[:15000]\n",
    "val_y = y_train[:15000]\n",
    "\n",
    "x_train = x_train[15000:]\n",
    "y_train = y_train[15000:]\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         1500000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          11216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,513,169\n",
      "Trainable params: 1,513,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 15000  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 100   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "264/264 [==============================] - 5s 7ms/step - loss: 0.4392 - accuracy: 0.7917 - val_loss: 0.3315 - val_accuracy: 0.8548\n",
      "Epoch 2/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.2921 - accuracy: 0.8778 - val_loss: 0.3239 - val_accuracy: 0.8605\n",
      "Epoch 3/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.2290 - accuracy: 0.9098 - val_loss: 0.3388 - val_accuracy: 0.8586\n",
      "Epoch 4/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.1620 - accuracy: 0.9401 - val_loss: 0.3871 - val_accuracy: 0.8526\n",
      "Epoch 5/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.1054 - accuracy: 0.9645 - val_loss: 0.4572 - val_accuracy: 0.8481\n",
      "Epoch 6/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0694 - accuracy: 0.9780 - val_loss: 0.5305 - val_accuracy: 0.8447\n",
      "Epoch 7/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0481 - accuracy: 0.9859 - val_loss: 0.5850 - val_accuracy: 0.8410\n",
      "Epoch 8/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0351 - accuracy: 0.9898 - val_loss: 0.6685 - val_accuracy: 0.8390\n",
      "Epoch 9/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0275 - accuracy: 0.9921 - val_loss: 0.7376 - val_accuracy: 0.8411\n",
      "Epoch 10/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0227 - accuracy: 0.9934 - val_loss: 0.8081 - val_accuracy: 0.8375\n",
      "Epoch 11/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.8310 - val_accuracy: 0.8371\n",
      "Epoch 12/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.9116 - val_accuracy: 0.8344\n",
      "Epoch 13/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0189 - accuracy: 0.9940 - val_loss: 0.8926 - val_accuracy: 0.8352\n",
      "Epoch 14/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0212 - accuracy: 0.9928 - val_loss: 1.0098 - val_accuracy: 0.8306\n",
      "Epoch 15/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 0.9439 - val_accuracy: 0.8312\n",
      "Epoch 16/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0233 - accuracy: 0.9919 - val_loss: 0.9748 - val_accuracy: 0.8347\n",
      "Epoch 17/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0157 - accuracy: 0.9949 - val_loss: 1.0116 - val_accuracy: 0.8326\n",
      "Epoch 18/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 1.0610 - val_accuracy: 0.8343\n",
      "Epoch 19/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0103 - accuracy: 0.9967 - val_loss: 1.1042 - val_accuracy: 0.8313\n",
      "Epoch 20/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0100 - accuracy: 0.9967 - val_loss: 1.1081 - val_accuracy: 0.8323\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_x, val_y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
    "from tensorflow.keras import optimizers, Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         1500000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 128)         117248    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 128)         82432     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,794,529\n",
      "Trainable params: 1,794,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         1500000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          11216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,513,169\n",
      "Trainable params: 1,513,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 15000  # 어휘 사전의 크기입니다\n",
    "word_vector_dim = 100   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_lstm.add(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.LSTM(128))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "model_lstm.summary()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0097 - accuracy: 0.9965 - val_loss: 1.2456 - val_accuracy: 0.8300\n",
      "Epoch 2/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0097 - accuracy: 0.9962 - val_loss: 1.3100 - val_accuracy: 0.8280\n",
      "Epoch 3/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0089 - accuracy: 0.9963 - val_loss: 1.3274 - val_accuracy: 0.8305\n",
      "Epoch 4/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0092 - accuracy: 0.9965 - val_loss: 1.3809 - val_accuracy: 0.8278\n",
      "Epoch 5/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0094 - accuracy: 0.9965 - val_loss: 1.3872 - val_accuracy: 0.8267\n",
      "Epoch 6/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0083 - accuracy: 0.9967 - val_loss: 1.3700 - val_accuracy: 0.8287\n",
      "Epoch 7/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0069 - accuracy: 0.9970 - val_loss: 1.4588 - val_accuracy: 0.8277\n",
      "Epoch 8/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0059 - accuracy: 0.9976 - val_loss: 1.4705 - val_accuracy: 0.8271\n",
      "Epoch 9/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0052 - accuracy: 0.9977 - val_loss: 1.5280 - val_accuracy: 0.8302\n",
      "Epoch 10/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0053 - accuracy: 0.9978 - val_loss: 1.5436 - val_accuracy: 0.8251\n",
      "Epoch 11/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0060 - accuracy: 0.9974 - val_loss: 1.5283 - val_accuracy: 0.8269\n",
      "Epoch 12/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0077 - accuracy: 0.9970 - val_loss: 1.4903 - val_accuracy: 0.8276\n",
      "Epoch 13/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0134 - accuracy: 0.9952 - val_loss: 1.4201 - val_accuracy: 0.8267\n",
      "Epoch 14/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 1.3261 - val_accuracy: 0.8274\n",
      "Epoch 15/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0075 - accuracy: 0.9970 - val_loss: 1.4391 - val_accuracy: 0.8313\n",
      "Epoch 16/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0055 - accuracy: 0.9978 - val_loss: 1.4859 - val_accuracy: 0.8298\n",
      "Epoch 17/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0047 - accuracy: 0.9979 - val_loss: 1.4994 - val_accuracy: 0.8292\n",
      "Epoch 18/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 1.5071 - val_accuracy: 0.8294\n",
      "Epoch 19/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0043 - accuracy: 0.9979 - val_loss: 1.4770 - val_accuracy: 0.8303\n",
      "Epoch 20/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0043 - accuracy: 0.9980 - val_loss: 1.5160 - val_accuracy: 0.8303\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_x, val_y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 100)         1500000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 128)         88320     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 128)         62208     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               62208     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,725,153\n",
      "Trainable params: 1,725,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         1500000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          11216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,513,169\n",
      "Trainable params: 1,513,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 15000  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 100   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_lstm.add(tf.keras.layers.GRU(128, return_sequences=True))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.GRU(128, return_sequences=True))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.GRU(128))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "model_lstm.summary()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 1.5425 - val_accuracy: 0.8289\n",
      "Epoch 2/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0069 - accuracy: 0.9970 - val_loss: 1.5252 - val_accuracy: 0.8324\n",
      "Epoch 3/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0060 - accuracy: 0.9974 - val_loss: 1.5741 - val_accuracy: 0.8283\n",
      "Epoch 4/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0048 - accuracy: 0.9979 - val_loss: 1.6982 - val_accuracy: 0.8242\n",
      "Epoch 5/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0053 - accuracy: 0.9977 - val_loss: 1.6522 - val_accuracy: 0.8278\n",
      "Epoch 6/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0047 - accuracy: 0.9977 - val_loss: 1.6723 - val_accuracy: 0.8310\n",
      "Epoch 7/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0047 - accuracy: 0.9979 - val_loss: 1.6637 - val_accuracy: 0.8308\n",
      "Epoch 8/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0048 - accuracy: 0.9980 - val_loss: 1.7063 - val_accuracy: 0.8264\n",
      "Epoch 9/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0044 - accuracy: 0.9980 - val_loss: 1.6932 - val_accuracy: 0.8294\n",
      "Epoch 10/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0040 - accuracy: 0.9979 - val_loss: 1.7623 - val_accuracy: 0.8280\n",
      "Epoch 11/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 1.7725 - val_accuracy: 0.8287\n",
      "Epoch 12/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0040 - accuracy: 0.9982 - val_loss: 1.8607 - val_accuracy: 0.8272\n",
      "Epoch 13/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0090 - accuracy: 0.9964 - val_loss: 1.7209 - val_accuracy: 0.8262\n",
      "Epoch 14/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0159 - accuracy: 0.9943 - val_loss: 1.5117 - val_accuracy: 0.8272\n",
      "Epoch 15/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0073 - accuracy: 0.9968 - val_loss: 1.5215 - val_accuracy: 0.8283\n",
      "Epoch 16/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0042 - accuracy: 0.9980 - val_loss: 1.5786 - val_accuracy: 0.8269\n",
      "Epoch 17/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0037 - accuracy: 0.9982 - val_loss: 1.5912 - val_accuracy: 0.8279\n",
      "Epoch 18/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0033 - accuracy: 0.9983 - val_loss: 1.6546 - val_accuracy: 0.8271\n",
      "Epoch 19/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0033 - accuracy: 0.9983 - val_loss: 1.6486 - val_accuracy: 0.8263\n",
      "Epoch 20/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0033 - accuracy: 0.9983 - val_loss: 1.6787 - val_accuracy: 0.8257\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_x, val_y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim 활용한 유사 단어 차이 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "def showGraph(bargraph):\n",
    "     xtick = [item[0] for item in bargraph] # 단어\n",
    "     ytick = [item[1] for item in bargraph] # 유사도\n",
    "     plt.figure()\n",
    "     mycolors = ['#06c2ac', '#c79fef', '#ff796c', '#aaff32', '#0485d1', '#d648d7', '#a5a502', '#d8dcd6', '#5ca904', '#fffe7a' ]\n",
    "     plt.bar(xtick, ytick, color=mycolors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리뷰데이터로 학습 후 유사도 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,word2vec\n",
    "model = Word2Vec(sentences=x_train_word, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bargraph = model.wv.most_similar(positive=['국민'],topn=10)\n",
    "showGraph(bargraph)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전학습 가중치로 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "model_filename = '../data/word2vec_ko.model'\n",
    "model = word2vec.Word2Vec.load(model_filename)\n",
    "\n",
    "# 유사도 구하기\n",
    "# 국민이라는 단어와 유사도가 높은 단어 10개를 리스트로 반환\n",
    "bargraph = model.wv.most_similar(positive=['국민'],topn=10)\n",
    "showGraph(bargraph)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전학습 가중치 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = word2vec.Word2Vec.load(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         30296000  \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 128)         88320     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 128)         62208     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               62208     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 30,521,153\n",
      "Trainable params: 30,521,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 302960  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 100   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model_gru = tf.keras.Sequential()\n",
    "model_gru.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,),weights=[w2v.wv.vectors]))\n",
    "model_gru.add(tf.keras.layers.GRU(128, return_sequences=True))\n",
    "model_gru.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_gru.add(tf.keras.layers.GRU(128, return_sequences=True))\n",
    "model_gru.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_gru.add(tf.keras.layers.GRU(128))\n",
    "model_gru.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_gru.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "264/264 [==============================] - 16s 40ms/step - loss: 0.4774 - accuracy: 0.7598 - val_loss: 0.4152 - val_accuracy: 0.8039\n",
      "Epoch 2/5\n",
      "264/264 [==============================] - 10s 36ms/step - loss: 0.3605 - accuracy: 0.8387 - val_loss: 0.3669 - val_accuracy: 0.8381\n",
      "Epoch 3/5\n",
      "264/264 [==============================] - 10s 36ms/step - loss: 0.3164 - accuracy: 0.8627 - val_loss: 0.3436 - val_accuracy: 0.8501\n",
      "Epoch 4/5\n",
      "264/264 [==============================] - 10s 37ms/step - loss: 0.2830 - accuracy: 0.8809 - val_loss: 0.3447 - val_accuracy: 0.8495\n",
      "Epoch 5/5\n",
      "264/264 [==============================] - 10s 37ms/step - loss: 0.2516 - accuracy: 0.8969 - val_loss: 0.3364 - val_accuracy: 0.8556\n"
     ]
    }
   ],
   "source": [
    "model_gru.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model_gru.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_x, val_y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
