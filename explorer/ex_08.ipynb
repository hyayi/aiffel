{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.3\n",
      "0.5.2\n",
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import konlpy\n",
    "import gensim\n",
    "\n",
    "print(pandas.__version__)\n",
    "print(konlpy.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=15000):\n",
    "  # 데이터 전처리(가공)후에 다시 train, test 데이터로 나눠주기 위해 원래 train_data의 크기를 기억\n",
    "  len_train = len(train_data)\n",
    "  \n",
    "  all_data = pd.concat([train_data, test_data], axis=0, ignore_index = True)\n",
    "  # 중복치와 결측치 제거\n",
    "  all_data = all_data.drop_duplicates('document', keep='first')\n",
    "  all_data = all_data.dropna(axis=0)\n",
    "  \n",
    "  # 후기 문장들을 sentence_list에 list로 저장\n",
    "  sentence_list = list(all_data['document'])\n",
    "  # 형태소로 나눈 문장들을 리스트에 따로 저장\n",
    "  token_list = []\n",
    "  for sentence in sentence_list:\n",
    "    tokenize_sentence = tokenizer.morphs(sentence)\n",
    "    tokenize_sentence = [word for word in tokenize_sentence if word not in stopwords] # 불용어 제거\n",
    "    token_list.append(tokenize_sentence)\n",
    "  # 토큰화한 문장들을 train, test로 나눠줌\n",
    "  x_train = token_list[:len_train]\n",
    "  x_test = token_list[len_train:]\n",
    "  \n",
    "  words = np.concatenate(x_train).tolist()\n",
    "  counter = Counter(words)\n",
    "  counter = counter.most_common(15000 - 4) # <pad>, <bos>, <unk>, <unused> 토큰을 추가해야하기 때문에 4개를 빼줌\n",
    "  vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "  \n",
    "  word2index = {word : index for index, word in enumerate(vocab)} # 단어를 정수 인덱스로 바꿔줌\n",
    "  \n",
    "  def wordlist_to_indexlist(wordlist):\n",
    "    return [word2index[word] if word in word2index else word2index['<UNK>'] for word in wordlist]\n",
    "  \n",
    "  x_train_index = list(map(wordlist_to_indexlist, x_train))\n",
    "  x_test_index = list(map(wordlist_to_indexlist, x_test))\n",
    "  \n",
    "  return x_train_index, np.array(list(all_data['label'][:len_train])), x_test_index, np.array(list(all_data['label'][len_train:])), word2index, x_train, x_test\n",
    "  \n",
    "x_train, y_train, x_test, y_test, word2index = load_data(train_data, test_data)\n",
    "index2word = {index : word for word, index in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  16.0182376132783\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.842655296434431\n",
      "pad_sequences maxlen :  41\n",
      "전체 문장의 0.9340557100486782%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "\n",
    "# 텍스트데이터 문장길이의 리스트를 생성\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산\n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 최대 길이를 (평균 + 2 * 표준편차) 로 한다면 남는 문장의 비율은?\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 41)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 문장의 마지막 입력이 최종 state에 영향을 가장 크게 미치기 때문에 패딩 적용은 pre(앞쪽)으로 해줍니다.\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word2index['<PAD>'],\n",
    "                                                        padding = 'pre',\n",
    "                                                        maxlen = maxlen)\n",
    " \n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word2index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135000, 41) (135000,)\n"
     ]
    }
   ],
   "source": [
    "val_x = x_train[:15000]\n",
    "val_y = y_train[:15000]\n",
    "\n",
    "x_train = x_train[15000:]\n",
    "y_train = y_train[15000:]\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         1500000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          11216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,513,169\n",
      "Trainable params: 1,513,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 15000  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 100   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "264/264 [==============================] - 5s 7ms/step - loss: 0.4392 - accuracy: 0.7917 - val_loss: 0.3315 - val_accuracy: 0.8548\n",
      "Epoch 2/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.2921 - accuracy: 0.8778 - val_loss: 0.3239 - val_accuracy: 0.8605\n",
      "Epoch 3/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.2290 - accuracy: 0.9098 - val_loss: 0.3388 - val_accuracy: 0.8586\n",
      "Epoch 4/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.1620 - accuracy: 0.9401 - val_loss: 0.3871 - val_accuracy: 0.8526\n",
      "Epoch 5/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.1054 - accuracy: 0.9645 - val_loss: 0.4572 - val_accuracy: 0.8481\n",
      "Epoch 6/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0694 - accuracy: 0.9780 - val_loss: 0.5305 - val_accuracy: 0.8447\n",
      "Epoch 7/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0481 - accuracy: 0.9859 - val_loss: 0.5850 - val_accuracy: 0.8410\n",
      "Epoch 8/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0351 - accuracy: 0.9898 - val_loss: 0.6685 - val_accuracy: 0.8390\n",
      "Epoch 9/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0275 - accuracy: 0.9921 - val_loss: 0.7376 - val_accuracy: 0.8411\n",
      "Epoch 10/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0227 - accuracy: 0.9934 - val_loss: 0.8081 - val_accuracy: 0.8375\n",
      "Epoch 11/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.8310 - val_accuracy: 0.8371\n",
      "Epoch 12/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.9116 - val_accuracy: 0.8344\n",
      "Epoch 13/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0189 - accuracy: 0.9940 - val_loss: 0.8926 - val_accuracy: 0.8352\n",
      "Epoch 14/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0212 - accuracy: 0.9928 - val_loss: 1.0098 - val_accuracy: 0.8306\n",
      "Epoch 15/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 0.9439 - val_accuracy: 0.8312\n",
      "Epoch 16/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0233 - accuracy: 0.9919 - val_loss: 0.9748 - val_accuracy: 0.8347\n",
      "Epoch 17/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0157 - accuracy: 0.9949 - val_loss: 1.0116 - val_accuracy: 0.8326\n",
      "Epoch 18/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 1.0610 - val_accuracy: 0.8343\n",
      "Epoch 19/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0103 - accuracy: 0.9967 - val_loss: 1.1042 - val_accuracy: 0.8313\n",
      "Epoch 20/20\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 0.0100 - accuracy: 0.9967 - val_loss: 1.1081 - val_accuracy: 0.8323\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_x, val_y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
    "from tensorflow.keras import optimizers, Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         1500000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 128)         117248    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 128)         82432     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,794,529\n",
      "Trainable params: 1,794,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         1500000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          11216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,513,169\n",
      "Trainable params: 1,513,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 15000  # 어휘 사전의 크기입니다\n",
    "word_vector_dim = 100   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_lstm.add(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.LSTM(128))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "model_lstm.summary()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0097 - accuracy: 0.9965 - val_loss: 1.2456 - val_accuracy: 0.8300\n",
      "Epoch 2/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0097 - accuracy: 0.9962 - val_loss: 1.3100 - val_accuracy: 0.8280\n",
      "Epoch 3/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0089 - accuracy: 0.9963 - val_loss: 1.3274 - val_accuracy: 0.8305\n",
      "Epoch 4/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0092 - accuracy: 0.9965 - val_loss: 1.3809 - val_accuracy: 0.8278\n",
      "Epoch 5/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0094 - accuracy: 0.9965 - val_loss: 1.3872 - val_accuracy: 0.8267\n",
      "Epoch 6/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0083 - accuracy: 0.9967 - val_loss: 1.3700 - val_accuracy: 0.8287\n",
      "Epoch 7/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0069 - accuracy: 0.9970 - val_loss: 1.4588 - val_accuracy: 0.8277\n",
      "Epoch 8/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0059 - accuracy: 0.9976 - val_loss: 1.4705 - val_accuracy: 0.8271\n",
      "Epoch 9/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0052 - accuracy: 0.9977 - val_loss: 1.5280 - val_accuracy: 0.8302\n",
      "Epoch 10/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0053 - accuracy: 0.9978 - val_loss: 1.5436 - val_accuracy: 0.8251\n",
      "Epoch 11/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0060 - accuracy: 0.9974 - val_loss: 1.5283 - val_accuracy: 0.8269\n",
      "Epoch 12/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0077 - accuracy: 0.9970 - val_loss: 1.4903 - val_accuracy: 0.8276\n",
      "Epoch 13/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0134 - accuracy: 0.9952 - val_loss: 1.4201 - val_accuracy: 0.8267\n",
      "Epoch 14/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 1.3261 - val_accuracy: 0.8274\n",
      "Epoch 15/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0075 - accuracy: 0.9970 - val_loss: 1.4391 - val_accuracy: 0.8313\n",
      "Epoch 16/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0055 - accuracy: 0.9978 - val_loss: 1.4859 - val_accuracy: 0.8298\n",
      "Epoch 17/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0047 - accuracy: 0.9979 - val_loss: 1.4994 - val_accuracy: 0.8292\n",
      "Epoch 18/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 1.5071 - val_accuracy: 0.8294\n",
      "Epoch 19/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0043 - accuracy: 0.9979 - val_loss: 1.4770 - val_accuracy: 0.8303\n",
      "Epoch 20/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0043 - accuracy: 0.9980 - val_loss: 1.5160 - val_accuracy: 0.8303\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_x, val_y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 100)         1500000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 128)         88320     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 128)         62208     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               62208     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,725,153\n",
      "Trainable params: 1,725,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         1500000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          11216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,513,169\n",
      "Trainable params: 1,513,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 15000  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 100   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_lstm.add(tf.keras.layers.GRU(128, return_sequences=True))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.GRU(128, return_sequences=True))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.GRU(128))\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "model_lstm.summary()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 1.5425 - val_accuracy: 0.8289\n",
      "Epoch 2/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0069 - accuracy: 0.9970 - val_loss: 1.5252 - val_accuracy: 0.8324\n",
      "Epoch 3/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0060 - accuracy: 0.9974 - val_loss: 1.5741 - val_accuracy: 0.8283\n",
      "Epoch 4/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0048 - accuracy: 0.9979 - val_loss: 1.6982 - val_accuracy: 0.8242\n",
      "Epoch 5/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0053 - accuracy: 0.9977 - val_loss: 1.6522 - val_accuracy: 0.8278\n",
      "Epoch 6/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0047 - accuracy: 0.9977 - val_loss: 1.6723 - val_accuracy: 0.8310\n",
      "Epoch 7/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0047 - accuracy: 0.9979 - val_loss: 1.6637 - val_accuracy: 0.8308\n",
      "Epoch 8/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0048 - accuracy: 0.9980 - val_loss: 1.7063 - val_accuracy: 0.8264\n",
      "Epoch 9/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0044 - accuracy: 0.9980 - val_loss: 1.6932 - val_accuracy: 0.8294\n",
      "Epoch 10/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0040 - accuracy: 0.9979 - val_loss: 1.7623 - val_accuracy: 0.8280\n",
      "Epoch 11/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 1.7725 - val_accuracy: 0.8287\n",
      "Epoch 12/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0040 - accuracy: 0.9982 - val_loss: 1.8607 - val_accuracy: 0.8272\n",
      "Epoch 13/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0090 - accuracy: 0.9964 - val_loss: 1.7209 - val_accuracy: 0.8262\n",
      "Epoch 14/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0159 - accuracy: 0.9943 - val_loss: 1.5117 - val_accuracy: 0.8272\n",
      "Epoch 15/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0073 - accuracy: 0.9968 - val_loss: 1.5215 - val_accuracy: 0.8283\n",
      "Epoch 16/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0042 - accuracy: 0.9980 - val_loss: 1.5786 - val_accuracy: 0.8269\n",
      "Epoch 17/20\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0037 - accuracy: 0.9982 - val_loss: 1.5912 - val_accuracy: 0.8279\n",
      "Epoch 18/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0033 - accuracy: 0.9983 - val_loss: 1.6546 - val_accuracy: 0.8271\n",
      "Epoch 19/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0033 - accuracy: 0.9983 - val_loss: 1.6486 - val_accuracy: 0.8263\n",
      "Epoch 20/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0033 - accuracy: 0.9983 - val_loss: 1.6787 - val_accuracy: 0.8257\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_x, val_y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim 활용한 유사 단어 차이 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def showGraph(bargraph):\n",
    "\n",
    "     xtick = [item[0] for item in bargraph] # 단어\n",
    "     ytick = [item[1] for item in bargraph] # 유사도\n",
    "     plt.figure()\n",
    "     mycolors = ['#06c2ac', '#c79fef', '#ff796c', '#aaff32', '#0485d1', '#d648d7', '#a5a502', '#d8dcd6', '#5ca904', '#fffe7a' ]\n",
    "     plt.bar(xtick, ytick, color=mycolors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,word2vec\n",
    "model = Word2Vec(sentences=x_train.tolist(), vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 4: 1,\n",
       " 2: 2,\n",
       " 5: 3,\n",
       " 6: 4,\n",
       " 7: 5,\n",
       " 8: 6,\n",
       " 9: 7,\n",
       " 10: 8,\n",
       " 11: 9,\n",
       " 12: 10,\n",
       " 13: 11,\n",
       " 14: 12,\n",
       " 15: 13,\n",
       " 16: 14,\n",
       " 17: 15,\n",
       " 18: 16,\n",
       " 19: 17,\n",
       " 20: 18,\n",
       " 21: 19,\n",
       " 22: 20,\n",
       " 23: 21,\n",
       " 24: 22,\n",
       " 25: 23,\n",
       " 27: 24,\n",
       " 26: 25,\n",
       " 28: 26,\n",
       " 30: 27,\n",
       " 29: 28,\n",
       " 31: 29,\n",
       " 33: 30,\n",
       " 32: 31,\n",
       " 34: 32,\n",
       " 35: 33,\n",
       " 36: 34,\n",
       " 37: 35,\n",
       " 38: 36,\n",
       " 39: 37,\n",
       " 40: 38,\n",
       " 42: 39,\n",
       " 43: 40,\n",
       " 41: 41,\n",
       " 44: 42,\n",
       " 46: 43,\n",
       " 45: 44,\n",
       " 47: 45,\n",
       " 48: 46,\n",
       " 50: 47,\n",
       " 49: 48,\n",
       " 52: 49,\n",
       " 51: 50,\n",
       " 53: 51,\n",
       " 54: 52,\n",
       " 57: 53,\n",
       " 56: 54,\n",
       " 55: 55,\n",
       " 58: 56,\n",
       " 59: 57,\n",
       " 61: 58,\n",
       " 60: 59,\n",
       " 64: 60,\n",
       " 63: 61,\n",
       " 70: 62,\n",
       " 62: 63,\n",
       " 66: 64,\n",
       " 68: 65,\n",
       " 69: 66,\n",
       " 67: 67,\n",
       " 65: 68,\n",
       " 74: 69,\n",
       " 78: 70,\n",
       " 72: 71,\n",
       " 73: 72,\n",
       " 75: 73,\n",
       " 77: 74,\n",
       " 71: 75,\n",
       " 80: 76,\n",
       " 76: 77,\n",
       " 81: 78,\n",
       " 79: 79,\n",
       " 82: 80,\n",
       " 83: 81,\n",
       " 84: 82,\n",
       " 85: 83,\n",
       " 89: 84,\n",
       " 86: 85,\n",
       " 87: 86,\n",
       " 88: 87,\n",
       " 91: 88,\n",
       " 90: 89,\n",
       " 94: 90,\n",
       " 92: 91,\n",
       " 93: 92,\n",
       " 96: 93,\n",
       " 98: 94,\n",
       " 95: 95,\n",
       " 97: 96,\n",
       " 102: 97,\n",
       " 100: 98,\n",
       " 101: 99,\n",
       " 99: 100,\n",
       " 105: 101,\n",
       " 103: 102,\n",
       " 104: 103,\n",
       " 106: 104,\n",
       " 107: 105,\n",
       " 108: 106,\n",
       " 111: 107,\n",
       " 110: 108,\n",
       " 115: 109,\n",
       " 112: 110,\n",
       " 109: 111,\n",
       " 114: 112,\n",
       " 113: 113,\n",
       " 116: 114,\n",
       " 117: 115,\n",
       " 118: 116,\n",
       " 119: 117,\n",
       " 120: 118,\n",
       " 121: 119,\n",
       " 124: 120,\n",
       " 123: 121,\n",
       " 129: 122,\n",
       " 122: 123,\n",
       " 125: 124,\n",
       " 126: 125,\n",
       " 130: 126,\n",
       " 127: 127,\n",
       " 128: 128,\n",
       " 134: 129,\n",
       " 136: 130,\n",
       " 133: 131,\n",
       " 132: 132,\n",
       " 131: 133,\n",
       " 137: 134,\n",
       " 135: 135,\n",
       " 139: 136,\n",
       " 138: 137,\n",
       " 140: 138,\n",
       " 142: 139,\n",
       " 145: 140,\n",
       " 143: 141,\n",
       " 144: 142,\n",
       " 146: 143,\n",
       " 141: 144,\n",
       " 147: 145,\n",
       " 148: 146,\n",
       " 151: 147,\n",
       " 149: 148,\n",
       " 153: 149,\n",
       " 154: 150,\n",
       " 155: 151,\n",
       " 157: 152,\n",
       " 150: 153,\n",
       " 158: 154,\n",
       " 156: 155,\n",
       " 152: 156,\n",
       " 159: 157,\n",
       " 161: 158,\n",
       " 167: 159,\n",
       " 162: 160,\n",
       " 166: 161,\n",
       " 163: 162,\n",
       " 160: 163,\n",
       " 164: 164,\n",
       " 176: 165,\n",
       " 170: 166,\n",
       " 171: 167,\n",
       " 169: 168,\n",
       " 173: 169,\n",
       " 172: 170,\n",
       " 165: 171,\n",
       " 174: 172,\n",
       " 168: 173,\n",
       " 178: 174,\n",
       " 175: 175,\n",
       " 177: 176,\n",
       " 180: 177,\n",
       " 179: 178,\n",
       " 186: 179,\n",
       " 184: 180,\n",
       " 183: 181,\n",
       " 189: 182,\n",
       " 181: 183,\n",
       " 182: 184,\n",
       " 185: 185,\n",
       " 193: 186,\n",
       " 191: 187,\n",
       " 188: 188,\n",
       " 202: 189,\n",
       " 190: 190,\n",
       " 194: 191,\n",
       " 195: 192,\n",
       " 197: 193,\n",
       " 187: 194,\n",
       " 201: 195,\n",
       " 192: 196,\n",
       " 199: 197,\n",
       " 200: 198,\n",
       " 196: 199,\n",
       " 205: 200,\n",
       " 198: 201,\n",
       " 203: 202,\n",
       " 206: 203,\n",
       " 204: 204,\n",
       " 211: 205,\n",
       " 208: 206,\n",
       " 210: 207,\n",
       " 207: 208,\n",
       " 216: 209,\n",
       " 209: 210,\n",
       " 215: 211,\n",
       " 218: 212,\n",
       " 229: 213,\n",
       " 212: 214,\n",
       " 213: 215,\n",
       " 226: 216,\n",
       " 214: 217,\n",
       " 221: 218,\n",
       " 222: 219,\n",
       " 220: 220,\n",
       " 223: 221,\n",
       " 217: 222,\n",
       " 232: 223,\n",
       " 228: 224,\n",
       " 230: 225,\n",
       " 224: 226,\n",
       " 227: 227,\n",
       " 219: 228,\n",
       " 234: 229,\n",
       " 231: 230,\n",
       " 225: 231,\n",
       " 233: 232,\n",
       " 240: 233,\n",
       " 238: 234,\n",
       " 239: 235,\n",
       " 241: 236,\n",
       " 245: 237,\n",
       " 255: 238,\n",
       " 257: 239,\n",
       " 235: 240,\n",
       " 237: 241,\n",
       " 244: 242,\n",
       " 246: 243,\n",
       " 243: 244,\n",
       " 258: 245,\n",
       " 247: 246,\n",
       " 249: 247,\n",
       " 250: 248,\n",
       " 260: 249,\n",
       " 251: 250,\n",
       " 242: 251,\n",
       " 236: 252,\n",
       " 248: 253,\n",
       " 253: 254,\n",
       " 252: 255,\n",
       " 259: 256,\n",
       " 254: 257,\n",
       " 264: 258,\n",
       " 256: 259,\n",
       " 265: 260,\n",
       " 263: 261,\n",
       " 262: 262,\n",
       " 266: 263,\n",
       " 273: 264,\n",
       " 272: 265,\n",
       " 270: 266,\n",
       " 271: 267,\n",
       " 267: 268,\n",
       " 261: 269,\n",
       " 268: 270,\n",
       " 269: 271,\n",
       " 280: 272,\n",
       " 278: 273,\n",
       " 287: 274,\n",
       " 282: 275,\n",
       " 289: 276,\n",
       " 281: 277,\n",
       " 284: 278,\n",
       " 274: 279,\n",
       " 279: 280,\n",
       " 275: 281,\n",
       " 283: 282,\n",
       " 294: 283,\n",
       " 286: 284,\n",
       " 277: 285,\n",
       " 285: 286,\n",
       " 288: 287,\n",
       " 276: 288,\n",
       " 290: 289,\n",
       " 301: 290,\n",
       " 295: 291,\n",
       " 297: 292,\n",
       " 291: 293,\n",
       " 303: 294,\n",
       " 296: 295,\n",
       " 309: 296,\n",
       " 300: 297,\n",
       " 292: 298,\n",
       " 302: 299,\n",
       " 314: 300,\n",
       " 308: 301,\n",
       " 310: 302,\n",
       " 293: 303,\n",
       " 298: 304,\n",
       " 312: 305,\n",
       " 313: 306,\n",
       " 307: 307,\n",
       " 304: 308,\n",
       " 305: 309,\n",
       " 315: 310,\n",
       " 311: 311,\n",
       " 317: 312,\n",
       " 318: 313,\n",
       " 316: 314,\n",
       " 306: 315,\n",
       " 319: 316,\n",
       " 321: 317,\n",
       " 320: 318,\n",
       " 299: 319,\n",
       " 337: 320,\n",
       " 328: 321,\n",
       " 334: 322,\n",
       " 322: 323,\n",
       " 329: 324,\n",
       " 324: 325,\n",
       " 323: 326,\n",
       " 339: 327,\n",
       " 346: 328,\n",
       " 336: 329,\n",
       " 333: 330,\n",
       " 327: 331,\n",
       " 332: 332,\n",
       " 330: 333,\n",
       " 343: 334,\n",
       " 326: 335,\n",
       " 340: 336,\n",
       " 325: 337,\n",
       " 338: 338,\n",
       " 331: 339,\n",
       " 341: 340,\n",
       " 350: 341,\n",
       " 359: 342,\n",
       " 351: 343,\n",
       " 349: 344,\n",
       " 347: 345,\n",
       " 342: 346,\n",
       " 348: 347,\n",
       " 345: 348,\n",
       " 378: 349,\n",
       " 362: 350,\n",
       " 357: 351,\n",
       " 367: 352,\n",
       " 352: 353,\n",
       " 335: 354,\n",
       " 370: 355,\n",
       " 366: 356,\n",
       " 344: 357,\n",
       " 363: 358,\n",
       " 358: 359,\n",
       " 355: 360,\n",
       " 377: 361,\n",
       " 368: 362,\n",
       " 353: 363,\n",
       " 354: 364,\n",
       " 361: 365,\n",
       " 379: 366,\n",
       " 364: 367,\n",
       " 356: 368,\n",
       " 380: 369,\n",
       " 372: 370,\n",
       " 373: 371,\n",
       " 371: 372,\n",
       " 381: 373,\n",
       " 360: 374,\n",
       " 374: 375,\n",
       " 376: 376,\n",
       " 387: 377,\n",
       " 389: 378,\n",
       " 383: 379,\n",
       " 369: 380,\n",
       " 384: 381,\n",
       " 375: 382,\n",
       " 385: 383,\n",
       " 388: 384,\n",
       " 401: 385,\n",
       " 400: 386,\n",
       " 390: 387,\n",
       " 386: 388,\n",
       " 394: 389,\n",
       " 391: 390,\n",
       " 396: 391,\n",
       " 392: 392,\n",
       " 397: 393,\n",
       " 402: 394,\n",
       " 405: 395,\n",
       " 413: 396,\n",
       " 382: 397,\n",
       " 409: 398,\n",
       " 395: 399,\n",
       " 399: 400,\n",
       " 412: 401,\n",
       " 410: 402,\n",
       " 393: 403,\n",
       " 417: 404,\n",
       " 406: 405,\n",
       " 414: 406,\n",
       " 408: 407,\n",
       " 398: 408,\n",
       " 407: 409,\n",
       " 403: 410,\n",
       " 418: 411,\n",
       " 425: 412,\n",
       " 426: 413,\n",
       " 422: 414,\n",
       " 404: 415,\n",
       " 419: 416,\n",
       " 416: 417,\n",
       " 411: 418,\n",
       " 430: 419,\n",
       " 431: 420,\n",
       " 427: 421,\n",
       " 420: 422,\n",
       " 424: 423,\n",
       " 432: 424,\n",
       " 433: 425,\n",
       " 440: 426,\n",
       " 435: 427,\n",
       " 415: 428,\n",
       " 421: 429,\n",
       " 428: 430,\n",
       " 434: 431,\n",
       " 365: 432,\n",
       " 438: 433,\n",
       " 429: 434,\n",
       " 439: 435,\n",
       " 423: 436,\n",
       " 453: 437,\n",
       " 436: 438,\n",
       " 444: 439,\n",
       " 447: 440,\n",
       " 451: 441,\n",
       " 448: 442,\n",
       " 452: 443,\n",
       " 437: 444,\n",
       " 446: 445,\n",
       " 454: 446,\n",
       " 466: 447,\n",
       " 449: 448,\n",
       " 442: 449,\n",
       " 459: 450,\n",
       " 443: 451,\n",
       " 456: 452,\n",
       " 441: 453,\n",
       " 458: 454,\n",
       " 455: 455,\n",
       " 469: 456,\n",
       " 470: 457,\n",
       " 460: 458,\n",
       " 462: 459,\n",
       " 474: 460,\n",
       " 461: 461,\n",
       " 476: 462,\n",
       " 445: 463,\n",
       " 482: 464,\n",
       " 450: 465,\n",
       " 480: 466,\n",
       " 457: 467,\n",
       " 468: 468,\n",
       " 478: 469,\n",
       " 467: 470,\n",
       " 487: 471,\n",
       " 475: 472,\n",
       " 477: 473,\n",
       " 484: 474,\n",
       " 471: 475,\n",
       " 463: 476,\n",
       " 481: 477,\n",
       " 486: 478,\n",
       " 491: 479,\n",
       " 464: 480,\n",
       " 509: 481,\n",
       " 485: 482,\n",
       " 506: 483,\n",
       " 488: 484,\n",
       " 495: 485,\n",
       " 511: 486,\n",
       " 493: 487,\n",
       " 483: 488,\n",
       " 490: 489,\n",
       " 499: 490,\n",
       " 465: 491,\n",
       " 472: 492,\n",
       " 497: 493,\n",
       " 513: 494,\n",
       " 494: 495,\n",
       " 500: 496,\n",
       " 524: 497,\n",
       " 492: 498,\n",
       " 501: 499,\n",
       " 512: 500,\n",
       " 504: 501,\n",
       " 502: 502,\n",
       " 518: 503,\n",
       " 496: 504,\n",
       " 522: 505,\n",
       " 520: 506,\n",
       " 473: 507,\n",
       " 514: 508,\n",
       " 479: 509,\n",
       " 505: 510,\n",
       " 521: 511,\n",
       " 532: 512,\n",
       " 507: 513,\n",
       " 525: 514,\n",
       " 508: 515,\n",
       " 526: 516,\n",
       " 503: 517,\n",
       " 489: 518,\n",
       " 527: 519,\n",
       " 515: 520,\n",
       " 519: 521,\n",
       " 498: 522,\n",
       " 533: 523,\n",
       " 538: 524,\n",
       " 516: 525,\n",
       " 535: 526,\n",
       " 537: 527,\n",
       " 540: 528,\n",
       " 530: 529,\n",
       " 536: 530,\n",
       " 552: 531,\n",
       " 531: 532,\n",
       " 539: 533,\n",
       " 510: 534,\n",
       " 517: 535,\n",
       " 534: 536,\n",
       " 529: 537,\n",
       " 549: 538,\n",
       " 523: 539,\n",
       " 556: 540,\n",
       " 563: 541,\n",
       " 528: 542,\n",
       " 555: 543,\n",
       " 554: 544,\n",
       " 544: 545,\n",
       " 573: 546,\n",
       " 550: 547,\n",
       " 570: 548,\n",
       " 542: 549,\n",
       " 558: 550,\n",
       " 551: 551,\n",
       " 548: 552,\n",
       " 553: 553,\n",
       " 564: 554,\n",
       " 595: 555,\n",
       " 559: 556,\n",
       " 582: 557,\n",
       " 545: 558,\n",
       " 541: 559,\n",
       " 546: 560,\n",
       " 547: 561,\n",
       " 565: 562,\n",
       " 557: 563,\n",
       " 589: 564,\n",
       " 576: 565,\n",
       " 597: 566,\n",
       " 543: 567,\n",
       " 560: 568,\n",
       " 585: 569,\n",
       " 586: 570,\n",
       " 574: 571,\n",
       " 606: 572,\n",
       " 593: 573,\n",
       " 575: 574,\n",
       " 579: 575,\n",
       " 562: 576,\n",
       " 572: 577,\n",
       " 611: 578,\n",
       " 580: 579,\n",
       " 581: 580,\n",
       " 577: 581,\n",
       " 605: 582,\n",
       " 568: 583,\n",
       " 583: 584,\n",
       " 567: 585,\n",
       " 571: 586,\n",
       " 566: 587,\n",
       " 600: 588,\n",
       " 617: 589,\n",
       " 590: 590,\n",
       " 591: 591,\n",
       " 569: 592,\n",
       " 619: 593,\n",
       " 627: 594,\n",
       " 584: 595,\n",
       " 598: 596,\n",
       " 622: 597,\n",
       " 604: 598,\n",
       " 599: 599,\n",
       " 587: 600,\n",
       " 561: 601,\n",
       " 588: 602,\n",
       " 631: 603,\n",
       " 603: 604,\n",
       " 601: 605,\n",
       " 612: 606,\n",
       " 596: 607,\n",
       " 633: 608,\n",
       " 608: 609,\n",
       " 607: 610,\n",
       " 592: 611,\n",
       " 613: 612,\n",
       " 602: 613,\n",
       " 628: 614,\n",
       " 609: 615,\n",
       " 594: 616,\n",
       " 629: 617,\n",
       " 621: 618,\n",
       " 639: 619,\n",
       " 610: 620,\n",
       " 634: 621,\n",
       " 616: 622,\n",
       " 626: 623,\n",
       " 636: 624,\n",
       " 623: 625,\n",
       " 666: 626,\n",
       " 578: 627,\n",
       " 630: 628,\n",
       " 664: 629,\n",
       " 655: 630,\n",
       " 644: 631,\n",
       " 646: 632,\n",
       " 615: 633,\n",
       " 620: 634,\n",
       " 614: 635,\n",
       " 625: 636,\n",
       " 624: 637,\n",
       " 632: 638,\n",
       " 645: 639,\n",
       " 637: 640,\n",
       " 618: 641,\n",
       " 642: 642,\n",
       " 648: 643,\n",
       " 640: 644,\n",
       " 692: 645,\n",
       " 635: 646,\n",
       " 641: 647,\n",
       " 653: 648,\n",
       " 649: 649,\n",
       " 652: 650,\n",
       " 665: 651,\n",
       " 656: 652,\n",
       " 658: 653,\n",
       " 662: 654,\n",
       " 676: 655,\n",
       " 674: 656,\n",
       " 657: 657,\n",
       " 650: 658,\n",
       " 643: 659,\n",
       " 669: 660,\n",
       " 687: 661,\n",
       " 660: 662,\n",
       " 668: 663,\n",
       " 671: 664,\n",
       " 677: 665,\n",
       " 693: 666,\n",
       " 661: 667,\n",
       " 672: 668,\n",
       " 699: 669,\n",
       " 654: 670,\n",
       " 686: 671,\n",
       " 680: 672,\n",
       " 684: 673,\n",
       " 694: 674,\n",
       " 705: 675,\n",
       " 683: 676,\n",
       " 670: 677,\n",
       " 667: 678,\n",
       " 682: 679,\n",
       " 673: 680,\n",
       " 675: 681,\n",
       " 651: 682,\n",
       " 663: 683,\n",
       " 697: 684,\n",
       " 715: 685,\n",
       " 681: 686,\n",
       " 691: 687,\n",
       " 698: 688,\n",
       " 707: 689,\n",
       " 659: 690,\n",
       " 732: 691,\n",
       " 647: 692,\n",
       " 717: 693,\n",
       " 690: 694,\n",
       " 688: 695,\n",
       " 703: 696,\n",
       " 679: 697,\n",
       " 689: 698,\n",
       " 709: 699,\n",
       " 638: 700,\n",
       " 696: 701,\n",
       " 719: 702,\n",
       " 706: 703,\n",
       " 713: 704,\n",
       " 678: 705,\n",
       " 712: 706,\n",
       " 710: 707,\n",
       " 711: 708,\n",
       " 708: 709,\n",
       " 729: 710,\n",
       " 725: 711,\n",
       " 716: 712,\n",
       " 737: 713,\n",
       " 724: 714,\n",
       " 723: 715,\n",
       " 739: 716,\n",
       " 700: 717,\n",
       " 722: 718,\n",
       " 701: 719,\n",
       " 747: 720,\n",
       " 746: 721,\n",
       " 733: 722,\n",
       " 718: 723,\n",
       " 727: 724,\n",
       " 745: 725,\n",
       " 734: 726,\n",
       " 748: 727,\n",
       " 752: 728,\n",
       " 685: 729,\n",
       " 720: 730,\n",
       " 728: 731,\n",
       " 749: 732,\n",
       " 695: 733,\n",
       " 754: 734,\n",
       " 726: 735,\n",
       " 714: 736,\n",
       " 735: 737,\n",
       " 702: 738,\n",
       " 755: 739,\n",
       " 738: 740,\n",
       " 736: 741,\n",
       " 731: 742,\n",
       " 704: 743,\n",
       " 770: 744,\n",
       " 721: 745,\n",
       " 743: 746,\n",
       " 760: 747,\n",
       " 764: 748,\n",
       " 753: 749,\n",
       " 773: 750,\n",
       " 756: 751,\n",
       " 742: 752,\n",
       " 757: 753,\n",
       " 741: 754,\n",
       " 730: 755,\n",
       " 758: 756,\n",
       " 779: 757,\n",
       " 751: 758,\n",
       " 763: 759,\n",
       " 772: 760,\n",
       " 799: 761,\n",
       " 771: 762,\n",
       " 740: 763,\n",
       " 780: 764,\n",
       " 798: 765,\n",
       " 805: 766,\n",
       " 797: 767,\n",
       " 776: 768,\n",
       " 750: 769,\n",
       " 793: 770,\n",
       " 744: 771,\n",
       " 784: 772,\n",
       " 762: 773,\n",
       " 775: 774,\n",
       " 786: 775,\n",
       " 759: 776,\n",
       " 766: 777,\n",
       " 774: 778,\n",
       " 791: 779,\n",
       " 781: 780,\n",
       " 777: 781,\n",
       " 801: 782,\n",
       " 778: 783,\n",
       " 768: 784,\n",
       " 785: 785,\n",
       " 765: 786,\n",
       " 783: 787,\n",
       " 796: 788,\n",
       " 804: 789,\n",
       " 761: 790,\n",
       " 802: 791,\n",
       " 825: 792,\n",
       " 794: 793,\n",
       " 830: 794,\n",
       " 818: 795,\n",
       " 769: 796,\n",
       " 800: 797,\n",
       " 782: 798,\n",
       " 790: 799,\n",
       " 824: 800,\n",
       " 807: 801,\n",
       " 810: 802,\n",
       " 767: 803,\n",
       " 837: 804,\n",
       " 812: 805,\n",
       " 829: 806,\n",
       " 787: 807,\n",
       " 806: 808,\n",
       " 809: 809,\n",
       " 803: 810,\n",
       " 815: 811,\n",
       " 814: 812,\n",
       " 819: 813,\n",
       " 813: 814,\n",
       " 792: 815,\n",
       " 827: 816,\n",
       " 817: 817,\n",
       " 789: 818,\n",
       " 795: 819,\n",
       " 831: 820,\n",
       " 820: 821,\n",
       " 832: 822,\n",
       " 816: 823,\n",
       " 826: 824,\n",
       " 788: 825,\n",
       " 808: 826,\n",
       " 864: 827,\n",
       " 855: 828,\n",
       " 847: 829,\n",
       " 852: 830,\n",
       " 840: 831,\n",
       " 849: 832,\n",
       " 822: 833,\n",
       " 863: 834,\n",
       " 834: 835,\n",
       " 821: 836,\n",
       " 844: 837,\n",
       " 828: 838,\n",
       " 833: 839,\n",
       " 854: 840,\n",
       " 851: 841,\n",
       " 811: 842,\n",
       " 836: 843,\n",
       " 858: 844,\n",
       " 869: 845,\n",
       " 842: 846,\n",
       " 881: 847,\n",
       " 846: 848,\n",
       " 835: 849,\n",
       " 874: 850,\n",
       " 879: 851,\n",
       " 841: 852,\n",
       " 875: 853,\n",
       " 888: 854,\n",
       " 848: 855,\n",
       " 884: 856,\n",
       " 853: 857,\n",
       " 850: 858,\n",
       " 904: 859,\n",
       " 893: 860,\n",
       " 903: 861,\n",
       " 868: 862,\n",
       " 870: 863,\n",
       " 922: 864,\n",
       " 843: 865,\n",
       " 838: 866,\n",
       " 891: 867,\n",
       " 898: 868,\n",
       " 859: 869,\n",
       " 887: 870,\n",
       " 878: 871,\n",
       " 872: 872,\n",
       " 866: 873,\n",
       " 839: 874,\n",
       " 862: 875,\n",
       " 857: 876,\n",
       " 861: 877,\n",
       " 943: 878,\n",
       " 865: 879,\n",
       " 905: 880,\n",
       " 823: 881,\n",
       " 880: 882,\n",
       " 867: 883,\n",
       " 886: 884,\n",
       " 876: 885,\n",
       " 885: 886,\n",
       " 925: 887,\n",
       " 899: 888,\n",
       " 873: 889,\n",
       " 895: 890,\n",
       " 856: 891,\n",
       " 882: 892,\n",
       " 916: 893,\n",
       " 883: 894,\n",
       " 860: 895,\n",
       " 877: 896,\n",
       " 931: 897,\n",
       " 941: 898,\n",
       " 912: 899,\n",
       " 896: 900,\n",
       " 936: 901,\n",
       " 907: 902,\n",
       " 890: 903,\n",
       " 889: 904,\n",
       " 924: 905,\n",
       " 961: 906,\n",
       " 956: 907,\n",
       " 930: 908,\n",
       " 871: 909,\n",
       " 932: 910,\n",
       " 970: 911,\n",
       " 921: 912,\n",
       " 938: 913,\n",
       " 927: 914,\n",
       " 910: 915,\n",
       " 918: 916,\n",
       " 920: 917,\n",
       " 913: 918,\n",
       " 945: 919,\n",
       " 979: 920,\n",
       " 908: 921,\n",
       " 909: 922,\n",
       " 949: 923,\n",
       " 929: 924,\n",
       " 928: 925,\n",
       " 900: 926,\n",
       " 967: 927,\n",
       " 845: 928,\n",
       " 919: 929,\n",
       " 963: 930,\n",
       " 917: 931,\n",
       " 892: 932,\n",
       " 897: 933,\n",
       " 902: 934,\n",
       " 937: 935,\n",
       " 950: 936,\n",
       " 962: 937,\n",
       " 952: 938,\n",
       " 894: 939,\n",
       " 911: 940,\n",
       " 935: 941,\n",
       " 948: 942,\n",
       " 958: 943,\n",
       " 957: 944,\n",
       " 959: 945,\n",
       " 944: 946,\n",
       " 1000: 947,\n",
       " 972: 948,\n",
       " 1003: 949,\n",
       " 940: 950,\n",
       " 975: 951,\n",
       " 947: 952,\n",
       " 951: 953,\n",
       " 914: 954,\n",
       " 934: 955,\n",
       " 954: 956,\n",
       " 960: 957,\n",
       " 997: 958,\n",
       " 965: 959,\n",
       " 995: 960,\n",
       " 984: 961,\n",
       " 978: 962,\n",
       " 933: 963,\n",
       " 964: 964,\n",
       " 953: 965,\n",
       " 926: 966,\n",
       " 1018: 967,\n",
       " 906: 968,\n",
       " 973: 969,\n",
       " 977: 970,\n",
       " 901: 971,\n",
       " 969: 972,\n",
       " 939: 973,\n",
       " 946: 974,\n",
       " 981: 975,\n",
       " 915: 976,\n",
       " 968: 977,\n",
       " 1004: 978,\n",
       " 923: 979,\n",
       " 1006: 980,\n",
       " 974: 981,\n",
       " 985: 982,\n",
       " 971: 983,\n",
       " 955: 984,\n",
       " 1005: 985,\n",
       " 990: 986,\n",
       " 994: 987,\n",
       " 1064: 988,\n",
       " 980: 989,\n",
       " 996: 990,\n",
       " 1023: 991,\n",
       " 991: 992,\n",
       " 1001: 993,\n",
       " 986: 994,\n",
       " 1013: 995,\n",
       " 982: 996,\n",
       " 942: 997,\n",
       " 1014: 998,\n",
       " 998: 999,\n",
       " ...}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bargraph = model.wv.most_similar(positive=['국민'],topn=10)\n",
    "showGraph(bargraph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "model_filename = '../data/word2vec_ko.model'\n",
    "model = word2vec.Word2Vec.load(model_filename)\n",
    "\n",
    "# 유사도 구하기\n",
    "# 국민이라는 단어와 유사도가 높은 단어 10개를 리스트로 반환\n",
    "bargraph = model.wv.most_similar(positive=['국민'],topn=10)\n",
    "showGraph(bargraph)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전학습 가중치 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = word2vec.Word2Vec.load(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         30296000  \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, None, 128)         88320     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, None, 128)         62208     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, None, 32)          4128      \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 128)               62208     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 30,521,153\n",
      "Trainable params: 30,521,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 302960  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 100   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model_gru = tf.keras.Sequential()\n",
    "model_gru.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,),weights=[w2v.wv.vectors]))\n",
    "model_gru.add(tf.keras.layers.GRU(128, return_sequences=True))\n",
    "model_gru.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_gru.add(tf.keras.layers.GRU(128, return_sequences=True))\n",
    "model_gru.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_gru.add(tf.keras.layers.GRU(128))\n",
    "model_gru.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model_gru.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "264/264 [==============================] - 14s 40ms/step - loss: 0.4728 - accuracy: 0.7650 - val_loss: 0.3850 - val_accuracy: 0.8274\n",
      "Epoch 2/5\n",
      "264/264 [==============================] - 10s 37ms/step - loss: 0.3572 - accuracy: 0.8408 - val_loss: 0.3627 - val_accuracy: 0.8343\n",
      "Epoch 3/5\n",
      "264/264 [==============================] - 10s 37ms/step - loss: 0.3172 - accuracy: 0.8633 - val_loss: 0.3417 - val_accuracy: 0.8481\n",
      "Epoch 4/5\n",
      "264/264 [==============================] - 10s 37ms/step - loss: 0.2818 - accuracy: 0.8810 - val_loss: 0.3403 - val_accuracy: 0.8491\n",
      "Epoch 5/5\n",
      "264/264 [==============================] - 10s 37ms/step - loss: 0.2506 - accuracy: 0.8963 - val_loss: 0.3416 - val_accuracy: 0.8497\n"
     ]
    }
   ],
   "source": [
    "model_gru.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model_gru.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_x, val_y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
